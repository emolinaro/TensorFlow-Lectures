{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating human faces with [Adversarial Networks](https://en.wikipedia.org/wiki/Generative_adversarial_network)\n",
    "\n",
    "In this lecture we'll train a neural network to generate plausible human faces in all their subtlty: appearance, expression, accessories, etc. \n",
    "\n",
    "Based on https://github.com/Lasagne/Recipes/pull/94 .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers as L\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "plt.rcParams.update({'axes.titlesize': 'small'})\n",
    "\n",
    "from sklearn.datasets import load_digits\n",
    "from utils import reset_tf_session\n",
    "from utils import download_file\n",
    "from utils import ModelSaveCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url1 = 'http://www.cs.columbia.edu/CAVE/databases/pubfig/download/lfw_attributes.txt'\n",
    "filepath1 = '/Users/molinaro/.keras/datasets/lfw_attributes.txt'\n",
    "\n",
    "url2 = 'http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz'\n",
    "filepath2 = '/Users/molinaro/.keras/datasets/lfw-deepfunneled.tgz'\n",
    "\n",
    "url3 = 'http://vis-www.cs.umass.edu/lfw/lfw.tgz'\n",
    "filepath3 = '/Users/molinaro/.keras/datasets/lfw.tgz'\n",
    "\n",
    "download_file(url1, filepath1)\n",
    "download_file(url2, filepath2)\n",
    "download_file(url3, filepath3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import tqdm\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "ATTRS_NAME = filepath1 \n",
    "IMAGES_NAME = filepath2  \n",
    "RAW_IMAGES_NAME = filepath3 \n",
    "\n",
    "def decode_image_from_raw_bytes(raw_bytes):\n",
    "    img = cv2.imdecode(np.asarray(bytearray(raw_bytes), dtype=np.uint8), 1)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img\n",
    "\n",
    "def load_lfw_dataset(\n",
    "        use_raw=False,\n",
    "        dx=80, dy=80,\n",
    "        dimx=45, dimy=45):\n",
    "\n",
    "    # read attrs\n",
    "    df_attrs = pd.read_csv(ATTRS_NAME, sep='\\t', skiprows=1)\n",
    "    df_attrs = pd.DataFrame(df_attrs.iloc[:, :-1].values, columns=df_attrs.columns[1:])\n",
    "    imgs_with_attrs = set(map(tuple, df_attrs[[\"person\", \"imagenum\"]].values))\n",
    "\n",
    "    # read photos\n",
    "    all_photos = []\n",
    "    photo_ids = []\n",
    "\n",
    "    with tarfile.open(RAW_IMAGES_NAME if use_raw else IMAGES_NAME) as f:\n",
    "        for m in tqdm.tqdm_notebook(f.getmembers()):\n",
    "            if m.isfile() and m.name.endswith(\".jpg\"):\n",
    "                # prepare image\n",
    "                img = decode_image_from_raw_bytes(f.extractfile(m).read())\n",
    "                img = img[dy:-dy, dx:-dx]\n",
    "                img = cv2.resize(img, (dimx, dimy))\n",
    "                # parse person\n",
    "                fname = os.path.split(m.name)[-1]\n",
    "                fname_splitted = fname[:-4].replace('_', ' ').split()\n",
    "                person_id = ' '.join(fname_splitted[:-1])\n",
    "                photo_number = int(fname_splitted[-1])\n",
    "                if (person_id, photo_number) in imgs_with_attrs:\n",
    "                    all_photos.append(img)\n",
    "                    photo_ids.append({'person': person_id, 'imagenum': photo_number})\n",
    "\n",
    "    photo_ids = pd.DataFrame(photo_ids)\n",
    "    all_photos = np.stack(all_photos).astype('uint8')\n",
    "\n",
    "    # preserve photo_ids order\n",
    "    all_attrs = photo_ids.merge(df_attrs, on=('person', 'imagenum')).drop([\"person\", \"imagenum\"], axis=1)\n",
    "\n",
    "    return all_photos, all_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n",
    "#Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n",
    "\n",
    "data,attrs = load_lfw_dataset(dimx=36,dimy=36)\n",
    "\n",
    "#preprocess faces\n",
    "data = np.float32(data)/255.\n",
    "\n",
    "IMG_SHAPE = data.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print random image\n",
    "plt.imshow(data[np.random.randint(data.shape[0])], cmap=\"gray\", interpolation=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative adversarial nets 101\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/torch/torch.github.io/master/blog/_posts/images/model.png\" width=320px height=240px>\n",
    "\n",
    "Deep learning workflow: \n",
    "* build some network that generates the face (small image)\n",
    "* make up a __measure__ of __how good that face is__\n",
    "* optimize with gradient descent \n",
    "\n",
    "\n",
    "The only problem is: how can we tell well-generated faces from bad? \n",
    "\n",
    "__If we can't tell good faces from bad, we delegate it to yet another neural network!__\n",
    "\n",
    "That makes the two of them:\n",
    "* __G__enerator - takes random noise for inspiration and tries to generate a face sample. \n",
    "  * Let's call it __G__(z), where z is a gaussian noise.\n",
    "* __D__iscriminator - takes a face sample and tries to tell if it's great or fake. \n",
    "  * Predicts the probability of input image being a __real face__\n",
    "  * Let's call it __D__(x), x being an image.\n",
    "  * __D(x)__ is a predition for real image and __D(G(z))__ is prediction for the face made by generator.\n",
    "\n",
    "Before we dive into training them, let's construct the two networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_options = tf.GPUOptions(allow_growth=True,per_process_gpu_memory_fraction=0.333)\n",
    "s = tf.InteractiveSession(config=tf.ConfigProto(gpu_options=gpu_options))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "CODE_SIZE = 256\n",
    "\n",
    "generator = Sequential()\n",
    "generator.add(L.InputLayer([CODE_SIZE],name='noise'))\n",
    "generator.add(L.Dense(10*8*8, activation='elu'))\n",
    "\n",
    "generator.add(L.Reshape((8,8,10)))\n",
    "generator.add(L.Deconv2D(64,kernel_size=(5,5),activation='elu'))\n",
    "generator.add(L.Deconv2D(64,kernel_size=(5,5),activation='elu'))\n",
    "generator.add(L.UpSampling2D(size=(2,2)))\n",
    "generator.add(L.Deconv2D(32,kernel_size=3,activation='elu'))\n",
    "generator.add(L.Deconv2D(32,kernel_size=3,activation='elu'))\n",
    "generator.add(L.Deconv2D(32,kernel_size=3,activation='elu'))\n",
    "\n",
    "generator.add(L.Conv2D(3,kernel_size=3,activation=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert generator.output_shape[1:] == IMG_SHAPE, \"generator must output an image of shape %s, but instead it produces %s\"%(IMG_SHAPE,generator.output_shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "* Discriminator is your usual convolutional network with interlooping convolution and pooling layers\n",
    "* The network does not include dropout/batchnorm to avoid learning complications.\n",
    "* We also regularize the pre-output layer to prevent discriminator from being too certain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Sequential()\n",
    "\n",
    "discriminator.add(L.InputLayer(IMG_SHAPE))\n",
    "\n",
    "#<build discriminator body>\n",
    "discriminator.add(L.Conv2D(16,kernel_size=(3, 3),padding='same', activation='elu'))\n",
    "discriminator.add(L.Conv2D(32,kernel_size=(3, 3),padding='same', activation='elu'))\n",
    "discriminator.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "discriminator.add(L.Conv2D(32,kernel_size=(3, 3),padding='same', activation='elu'))\n",
    "discriminator.add(L.Conv2D(64,kernel_size=(3, 3),padding='same', activation='elu'))\n",
    "discriminator.add(L.MaxPooling2D(pool_size=(2, 2)))\n",
    "discriminator.add(L.Flatten())\n",
    "discriminator.add(L.Dense(256,activation='tanh'))\n",
    "discriminator.add(L.Dense(2,activation=tf.nn.log_softmax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We train the two networks concurrently:\n",
    "* Train __discriminator__ to better distinguish real data from __current__ generator\n",
    "* Train __generator__ to make discriminator think generator is real\n",
    "* Since discriminator is a differentiable neural network, we train both with gradient descent.\n",
    "\n",
    "Training is done iteratively until discriminator is no longer able to find the difference (or until you run out of patience).\n",
    "\n",
    "\n",
    "### Tricks:\n",
    "* Regularize discriminator output weights to prevent explosion\n",
    "* Train generator with __adam__ to speed up training. Discriminator trains with SGD to avoid problems with momentum.\n",
    "* More: https://github.com/soumith/ganhacks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.placeholder('float32',[None,CODE_SIZE])\n",
    "real_data = tf.placeholder('float32',[None,]+list(IMG_SHAPE))\n",
    "\n",
    "logp_real = discriminator(real_data)\n",
    "\n",
    "generated_data = generator(noise)\n",
    "\n",
    "logp_gen = discriminator(generated_data) #<log P(real | gen(noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "#discriminator training#\n",
    "########################\n",
    "\n",
    "d_loss = -tf.reduce_mean(logp_real[:,1] + logp_gen[:,0])\n",
    "\n",
    "#regularize\n",
    "d_loss += tf.reduce_mean(discriminator.layers[-1].kernel**2)\n",
    "\n",
    "#optimize\n",
    "disc_optimizer =  tf.train.GradientDescentOptimizer(1e-3).minimize(d_loss,var_list=discriminator.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "###generator training###\n",
    "########################\n",
    "\n",
    "g_loss = -tf.reduce_mean(logp_real[:,0] + logp_gen[:,1]) #<generator loss>\n",
    "\n",
    "gen_optimizer = tf.train.AdamOptimizer(1e-4).minimize(g_loss,var_list=generator.trainable_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary functions\n",
    "Here we define a few helper functions that draw current data distributions and sample training batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_noise_batch(bsize):\n",
    "    return np.random.normal(size=(bsize, CODE_SIZE)).astype('float32')\n",
    "\n",
    "def sample_data_batch(bsize):\n",
    "    idxs = np.random.choice(np.arange(data.shape[0]), size=bsize)\n",
    "    return data[idxs]\n",
    "\n",
    "def sample_images(nrow,ncol, sharp=False):\n",
    "    images = generator.predict(sample_noise_batch(bsize=nrow*ncol))\n",
    "    if np.var(images)!=0:\n",
    "        images = images.clip(np.min(data),np.max(data))\n",
    "    for i in range(nrow*ncol):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        if sharp:\n",
    "            plt.imshow(images[i].reshape(IMG_SHAPE),cmap=\"gray\", interpolation=\"none\")\n",
    "        else:\n",
    "            plt.imshow(images[i].reshape(IMG_SHAPE),cmap=\"gray\")\n",
    "    plt.show()\n",
    "\n",
    "def sample_probas(bsize):\n",
    "    plt.title('Generated vs real data')\n",
    "    plt.hist(np.exp(discriminator.predict(sample_data_batch(bsize)))[:,1],\n",
    "             label='D(x)', alpha=0.5,range=[0,1])\n",
    "    plt.hist(np.exp(discriminator.predict(generator.predict(sample_noise_batch(bsize))))[:,1],\n",
    "             label='D(G(z))',alpha=0.5,range=[0,1])\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "Main loop.\n",
    "We just train generator and discriminator in a loop and plot results once every N iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from tqdm import tnrange\n",
    "\n",
    "for epoch in tnrange(50000):\n",
    "    \n",
    "    feed_dict = {\n",
    "        real_data:sample_data_batch(100),\n",
    "        noise:sample_noise_batch(100)\n",
    "    }\n",
    "    \n",
    "    for i in range(5):\n",
    "        s.run(disc_optimizer,feed_dict)\n",
    "    \n",
    "    s.run(gen_optimizer,feed_dict)\n",
    "    \n",
    "    if epoch %100==0:\n",
    "        display.clear_output(wait=True)\n",
    "        sample_images(2,3,True)\n",
    "        sample_probas(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights to file\n",
    "discriminator.save_weights(\"weights_disc.h5\")\n",
    "generator.save_weights(\"weights_gen.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The network was trained for about 15k iterations. \n",
    "#Training for longer yields MUCH better results\n",
    "plt.figure(figsize=[16,24])\n",
    "sample_images(16,8)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
